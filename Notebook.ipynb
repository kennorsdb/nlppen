{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a834852-b9fd-483c-93e6-3b80f3e879c5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2ec801-fdee-413e-aca9-8ee8e77e0cac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from nlppen.extraccion.utils.Txt2Numbers import Txt2Numbers\n",
    "from nlppen.analisis import Analisis\n",
    "from nlppen.seleccion import Seleccion\n",
    "from nlppen.spark_udfs import solo_portanto, spark_get_spacy\n",
    "from nlppen.sentencias_estructurales import SentenciasEstructurales\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106f23a-2bb1-483d-bc31-aa48b9a7db2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialize spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b819481b-6ee8-42c7-9add-c19ceba43563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://69abb479a5e1:4041'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Transforming sentences\")\n",
    "         .config(\"spark.num.executors\", \"2\")\n",
    "         .config(\"spark.executor.memory\", \"10g\")\n",
    "         .config(\"spark.executor.cores\", \"4\")\n",
    "         .config(\"spark.driver.memory\", \"10g\")\n",
    "         .config(\"spark.cores.max\", \"12\")\n",
    "         .config(\"spark.memory.offHeap.enabled\", True)\n",
    "         .config(\"spark.memory.offHeap.size\", \"64g\")\n",
    "         .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab75fdc-6e47-4671-b06b-4109f7a2bc66",
   "metadata": {},
   "source": [
    "# Buscar terminos en la secci√≥n de por lo tanto de la sentencia y aplicar filtro del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a92cddd-4b24-4bdf-85d7-ceabe68d009d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "terminos = {\n",
    "    'seguimiento': [r'\\bseguimiento\\b'],\n",
    "    'se ordena': [r'\\bse ordena\\b'],\n",
    "    'plan': [r'\\bplan\\b'],\n",
    "    'plazo': [r'\\bplazo\\b']\n",
    "}\n",
    "seleccion = Seleccion(terminos, spark, parquet_path='../../src/datasets/complete', datasets_path='./datasets_stanza/')\n",
    "seleccion.filtrar_sentencias(preprocess=solo_portanto)\n",
    "\n",
    "estructurales = SentenciasEstructurales(seleccion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b9686-78ed-4c53-9cf9-694c588b45e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Formar dataset de sentencias estructurales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2efc878d-2fc1-4743-b8ea-c3fecf943042",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-23 22:29:28 INFO: Loading these models for language: es (Spanish): / 27]\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2021-09-23 22:29:28 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2021-09-23 22:29:28 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2021-09-23 22:29:28 INFO: Use device: gpu\n",
      "2021-09-23 22:29:28 INFO: Loading: tokenize\n",
      "2021-09-23 22:29:28 INFO: Use device: gpu\n",
      "2021-09-23 22:29:28 INFO: Loading: tokenize\n",
      "2021-09-23 22:29:28 INFO: Use device: gpu\n",
      "2021-09-23 22:29:28 INFO: Loading: tokenize\n",
      "2021-09-23 22:29:28 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2021-09-23 22:29:28 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2021-09-23 22:29:28 INFO: Use device: gpu\n",
      "2021-09-23 22:29:28 INFO: Loading: tokenize\n",
      "2021-09-23 22:29:28 INFO: Use device: gpu\n",
      "2021-09-23 22:29:28 INFO: Loading: tokenize\n",
      "2021-09-23 22:29:28 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2021-09-23 22:29:28 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2021-09-23 22:29:28 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2021-09-23 22:29:28 INFO: Use device: gpu\n",
      "2021-09-23 22:29:28 INFO: Loading: tokenize\n",
      "2021-09-23 22:29:28 INFO: Use device: gpu\n",
      "2021-09-23 22:29:28 INFO: Loading: tokenize\n",
      "2021-09-23 22:29:28 INFO: Use device: gpu\n",
      "2021-09-23 22:29:28 INFO: Loading: tokenize\n",
      "2021-09-23 22:29:29 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2021-09-23 22:29:29 INFO: Use device: gpu\n",
      "2021-09-23 22:29:29 INFO: Loading: tokenize\n",
      "2021-09-23 22:29:30 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2021-09-23 22:29:30 INFO: Use device: gpu\n",
      "2021-09-23 22:29:30 INFO: Loading: tokenize\n",
      "2021-09-23 22:29:30 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2021-09-23 22:29:30 INFO: Use device: gpu\n",
      "2021-09-23 22:29:30 INFO: Loading: tokenize\n",
      "2021-09-23 22:29:31 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2021-09-23 22:29:32 INFO: Use device: gpu\n",
      "2021-09-23 22:29:32 INFO: Loading: tokenize\n",
      "21/09/23 22:29:34 WARN BlockManager: Putting block rdd_135_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 89, in <lambda>\n",
      "    .map( lambda row : spark_extraer_entidades_se_ordena(row, newColumns , patterns, solo_portanto , useSpacy=spacy))\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 245, in spark_extraer_entidades_se_ordena\n",
      "    getEntitiesByStanza(textSpan.text, entities, newColumns)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 298, in getEntitiesByStanza\n",
      "    nlp = spark_get_stanza(\"es\")\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 115, in spark_get_stanza\n",
      "    nlpStanza = stanza.Pipeline(lang)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/core.py\", line 139, in __init__\n",
      "    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/processor.py\", line 159, in __init__\n",
      "    self._set_up_model(config, use_gpu)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/tokenize_processor.py\", line 40, in _set_up_model\n",
      "    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/models/tokenization/trainer.py\", line 27, in __init__\n",
      "    self.model.cuda()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in cuda\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 530, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 552, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in <lambda>\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "RuntimeError: CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      ".\n",
      "21/09/23 22:29:34 WARN BlockManager: Block rdd_135_0 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:34 WARN BlockManager: Putting block rdd_148_0 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 89, in <lambda>\n",
      "    .map( lambda row : spark_extraer_entidades_se_ordena(row, newColumns , patterns, solo_portanto , useSpacy=spacy))\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 245, in spark_extraer_entidades_se_ordena\n",
      "    getEntitiesByStanza(textSpan.text, entities, newColumns)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 298, in getEntitiesByStanza\n",
      "    nlp = spark_get_stanza(\"es\")\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 115, in spark_get_stanza\n",
      "    nlpStanza = stanza.Pipeline(lang)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/core.py\", line 139, in __init__\n",
      "    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/processor.py\", line 159, in __init__\n",
      "    self._set_up_model(config, use_gpu)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/tokenize_processor.py\", line 40, in _set_up_model\n",
      "    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/models/tokenization/trainer.py\", line 27, in __init__\n",
      "    self.model.cuda()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in cuda\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 530, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 552, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in <lambda>\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "RuntimeError: CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      ".\n",
      "21/09/23 22:29:34 WARN BlockManager: Block rdd_148_0 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:34 ERROR Executor: Exception in task 0.0 in stage 13.0 (TID 248)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 89, in <lambda>\n",
      "    .map( lambda row : spark_extraer_entidades_se_ordena(row, newColumns , patterns, solo_portanto , useSpacy=spacy))\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 245, in spark_extraer_entidades_se_ordena\n",
      "    getEntitiesByStanza(textSpan.text, entities, newColumns)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 298, in getEntitiesByStanza\n",
      "    nlp = spark_get_stanza(\"es\")\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 115, in spark_get_stanza\n",
      "    nlpStanza = stanza.Pipeline(lang)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/core.py\", line 139, in __init__\n",
      "    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/processor.py\", line 159, in __init__\n",
      "    self._set_up_model(config, use_gpu)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/tokenize_processor.py\", line 40, in _set_up_model\n",
      "    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/models/tokenization/trainer.py\", line 27, in __init__\n",
      "    self.model.cuda()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in cuda\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 530, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 552, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in <lambda>\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "RuntimeError: CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/09/23 22:29:35 WARN TaskSetManager: Lost task 0.0 in stage 13.0 (TID 248) (69abb479a5e1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 89, in <lambda>\n",
      "    .map( lambda row : spark_extraer_entidades_se_ordena(row, newColumns , patterns, solo_portanto , useSpacy=spacy))\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 245, in spark_extraer_entidades_se_ordena\n",
      "    getEntitiesByStanza(textSpan.text, entities, newColumns)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 298, in getEntitiesByStanza\n",
      "    nlp = spark_get_stanza(\"es\")\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 115, in spark_get_stanza\n",
      "    nlpStanza = stanza.Pipeline(lang)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/core.py\", line 139, in __init__\n",
      "    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/processor.py\", line 159, in __init__\n",
      "    self._set_up_model(config, use_gpu)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/tokenize_processor.py\", line 40, in _set_up_model\n",
      "    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/models/tokenization/trainer.py\", line 27, in __init__\n",
      "    self.model.cuda()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in cuda\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 530, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 552, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in <lambda>\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "RuntimeError: CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/09/23 22:29:35 ERROR TaskSetManager: Task 0 in stage 13.0 failed 1 times; aborting job\n",
      "21/09/23 22:29:38 ERROR FileFormatWriter: Aborting job 5090d953-5add-47f7-9902-28098193dd0c.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 248) (69abb479a5e1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 89, in <lambda>\n",
      "    .map( lambda row : spark_extraer_entidades_se_ordena(row, newColumns , patterns, solo_portanto , useSpacy=spacy))\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 245, in spark_extraer_entidades_se_ordena\n",
      "    getEntitiesByStanza(textSpan.text, entities, newColumns)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 298, in getEntitiesByStanza\n",
      "    nlp = spark_get_stanza(\"es\")\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 115, in spark_get_stanza\n",
      "    nlpStanza = stanza.Pipeline(lang)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/core.py\", line 139, in __init__\n",
      "    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/processor.py\", line 159, in __init__\n",
      "    self._set_up_model(config, use_gpu)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/tokenize_processor.py\", line 40, in _set_up_model\n",
      "    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/models/tokenization/trainer.py\", line 27, in __init__\n",
      "    self.model.cuda()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in cuda\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 530, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 552, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in <lambda>\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "RuntimeError: CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 89, in <lambda>\n",
      "    .map( lambda row : spark_extraer_entidades_se_ordena(row, newColumns , patterns, solo_portanto , useSpacy=spacy))\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 245, in spark_extraer_entidades_se_ordena\n",
      "    getEntitiesByStanza(textSpan.text, entities, newColumns)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 298, in getEntitiesByStanza\n",
      "    nlp = spark_get_stanza(\"es\")\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 115, in spark_get_stanza\n",
      "    nlpStanza = stanza.Pipeline(lang)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/core.py\", line 139, in __init__\n",
      "    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/processor.py\", line 159, in __init__\n",
      "    self._set_up_model(config, use_gpu)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/tokenize_processor.py\", line 40, in _set_up_model\n",
      "    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/stanza/models/tokenization/trainer.py\", line 27, in __init__\n",
      "    self.model.cuda()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in cuda\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 530, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 552, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in <lambda>\n",
      "    return self._apply(lambda t: t.cuda(device))\n",
      "RuntimeError: CUDA error: unknown error\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 22.0 in stage 13 (TID 270) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 8.0 in stage 13 (TID 256) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 3.0 in stage 13 (TID 251) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 6.0 in stage 13 (TID 254) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 2.0 in stage 13 (TID 250) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 20.0 in stage 13 (TID 268) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 1.0 in stage 13 (TID 249) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 5.0 in stage 13 (TID 253) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 17.0 in stage 13 (TID 265) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 7.0 in stage 13 (TID 255) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 4.0 in stage 13 (TID 252) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 16.0 in stage 13 (TID 264) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 11.0 in stage 13 (TID 259) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 9.0 in stage 13 (TID 257) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 18.0 in stage 13 (TID 266) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 12.0 in stage 13 (TID 260) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 23.0 in stage 13 (TID 271) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 14.0 in stage 13 (TID 262) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 13.0 in stage 13 (TID 261) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 24.0 in stage 13 (TID 272) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 10.0 in stage 13 (TID 258) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 19.0 in stage 13 (TID 267) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN PythonRunner: Incomplete task 21.0 in stage 13 (TID 269) interrupted: Attempting to kill Python Worker\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_20 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_135_5 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_135_2 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_135_6 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_135_3 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_148_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_148_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_148_2 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_148_5 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_135_4 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_148_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_148_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_135_1 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_135_20 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_19 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_24 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_10 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_135_10 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_148_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_148_3 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_135_24 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_24 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_24 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_148_4 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_22 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_17 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_9 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_12 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_135_19 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_16 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_11 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_18 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_148_10 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_148_20 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_14 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_13 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_135_13 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_13 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_13 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_148_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_148_6 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_1 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:39 WARN BlockManager: Putting block rdd_135_23 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:39 WARN BlockManager: Block rdd_135_7 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_135_14 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_135_23 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_135_17 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_20 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 24.0 in stage 13.0 (TID 272) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_135_9 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_135_18 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_135_16 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_135_11 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_135_8 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_19 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_10 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_135_22 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_135_12 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_135_21 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_23 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_23 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_135_21 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_14 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_19 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_14 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_17 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_17 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_7 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_12 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 5.0 in stage 13.0 (TID 253) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_12 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_16 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_11 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_22 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_21 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 2.0 in stage 13.0 (TID 250) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_16 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_22 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_11 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 13.0 in stage 13.0 (TID 261) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_21 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_18 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_18 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_8 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 6.0 in stage 13.0 (TID 254) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_9 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_9 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 10.0 in stage 13.0 (TID 258) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 20.0 in stage 13.0 (TID 268) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 4.0 in stage 13.0 (TID 252) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 3.0 in stage 13.0 (TID 251) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 1.0 in stage 13.0 (TID 249) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 18.0 in stage 13.0 (TID 266) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 14.0 in stage 13.0 (TID 262) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 19.0 in stage 13.0 (TID 267) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 22.0 in stage 13.0 (TID 270) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 8.0 in stage 13.0 (TID 256) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 23.0 in stage 13.0 (TID 271) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 21.0 in stage 13.0 (TID 269) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 9.0 in stage 13.0 (TID 257) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 11.0 in stage 13.0 (TID 259) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 16.0 in stage 13.0 (TID 264) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 17.0 in stage 13.0 (TID 265) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 7.0 in stage 13.0 (TID 255) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 12.0 in stage 13.0 (TID 260) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_135_25 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_135_25 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN BlockManager: Putting block rdd_148_25 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/09/23 22:29:40 WARN BlockManager: Block rdd_148_25 could not be removed as it was not found on disk or in memory\n",
      "21/09/23 22:29:40 WARN TaskSetManager: Lost task 25.0 in stage 13.0 (TID 273) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o340.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 248) (69abb479a5e1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 89, in <lambda>\n    .map( lambda row : spark_extraer_entidades_se_ordena(row, newColumns , patterns, solo_portanto , useSpacy=spacy))\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 245, in spark_extraer_entidades_se_ordena\n    getEntitiesByStanza(textSpan.text, entities, newColumns)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 298, in getEntitiesByStanza\n    nlp = spark_get_stanza(\"es\")\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 115, in spark_get_stanza\n    nlpStanza = stanza.Pipeline(lang)\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/core.py\", line 139, in __init__\n    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/processor.py\", line 159, in __init__\n    self._set_up_model(config, use_gpu)\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/tokenize_processor.py\", line 40, in _set_up_model\n    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/models/tokenization/trainer.py\", line 27, in __init__\n    self.model.cuda()\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in cuda\n    return self._apply(lambda t: t.cuda(device))\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 530, in _apply\n    module._apply(fn)\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 552, in _apply\n    param_applied = fn(param)\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in <lambda>\n    return self._apply(lambda t: t.cuda(device))\nRuntimeError: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\t... 33 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 89, in <lambda>\n    .map( lambda row : spark_extraer_entidades_se_ordena(row, newColumns , patterns, solo_portanto , useSpacy=spacy))\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 245, in spark_extraer_entidades_se_ordena\n    getEntitiesByStanza(textSpan.text, entities, newColumns)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 298, in getEntitiesByStanza\n    nlp = spark_get_stanza(\"es\")\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 115, in spark_get_stanza\n    nlpStanza = stanza.Pipeline(lang)\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/core.py\", line 139, in __init__\n    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/processor.py\", line 159, in __init__\n    self._set_up_model(config, use_gpu)\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/tokenize_processor.py\", line 40, in _set_up_model\n    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/models/tokenization/trainer.py\", line 27, in __init__\n    self.model.cuda()\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in cuda\n    return self._apply(lambda t: t.cuda(device))\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 530, in _apply\n    module._apply(fn)\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 552, in _apply\n    param_applied = fn(param)\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in <lambda>\n    return self._apply(lambda t: t.cuda(device))\nRuntimeError: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_92959/916602265.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#Sobreescribir el dataset de filtro de sentencias con las nuevas columnas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mestructurales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseleccion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguardarDatos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Work/ej/paquetes/nlppen/nlppen/seleccion.py\u001b[0m in \u001b[0;36mguardarDatos\u001b[0;34m(self, parquet_file)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                  \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcargarPreprocesados\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparquet_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'terminos.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o340.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 248) (69abb479a5e1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 89, in <lambda>\n    .map( lambda row : spark_extraer_entidades_se_ordena(row, newColumns , patterns, solo_portanto , useSpacy=spacy))\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 245, in spark_extraer_entidades_se_ordena\n    getEntitiesByStanza(textSpan.text, entities, newColumns)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 298, in getEntitiesByStanza\n    nlp = spark_get_stanza(\"es\")\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 115, in spark_get_stanza\n    nlpStanza = stanza.Pipeline(lang)\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/core.py\", line 139, in __init__\n    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/processor.py\", line 159, in __init__\n    self._set_up_model(config, use_gpu)\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/tokenize_processor.py\", line 40, in _set_up_model\n    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/models/tokenization/trainer.py\", line 27, in __init__\n    self.model.cuda()\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in cuda\n    return self._apply(lambda t: t.cuda(device))\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 530, in _apply\n    module._apply(fn)\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 552, in _apply\n    param_applied = fn(param)\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in <lambda>\n    return self._apply(lambda t: t.cuda(device))\nRuntimeError: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\t... 33 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 89, in <lambda>\n    .map( lambda row : spark_extraer_entidades_se_ordena(row, newColumns , patterns, solo_portanto , useSpacy=spacy))\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 245, in spark_extraer_entidades_se_ordena\n    getEntitiesByStanza(textSpan.text, entities, newColumns)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 298, in getEntitiesByStanza\n    nlp = spark_get_stanza(\"es\")\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 115, in spark_get_stanza\n    nlpStanza = stanza.Pipeline(lang)\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/core.py\", line 139, in __init__\n    self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/processor.py\", line 159, in __init__\n    self._set_up_model(config, use_gpu)\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/pipeline/tokenize_processor.py\", line 40, in _set_up_model\n    self._trainer = Trainer(model_file=config['model_path'], use_cuda=use_gpu)\n  File \"/opt/conda/lib/python3.9/site-packages/stanza/models/tokenization/trainer.py\", line 27, in __init__\n    self.model.cuda()\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in cuda\n    return self._apply(lambda t: t.cuda(device))\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 530, in _apply\n    module._apply(fn)\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 552, in _apply\n    param_applied = fn(param)\n  File \"/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 637, in <lambda>\n    return self._apply(lambda t: t.cuda(device))\nRuntimeError: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "columnas = {\n",
    "    'se ordena PER' : ArrayType(StringType()),\n",
    "    'se ordena LOC' : ArrayType(StringType()),\n",
    "    'se ordena ORG' : ArrayType(StringType()),\n",
    "    'se ordena MISC' : ArrayType(StringType()),\n",
    "    'se ordena GPE' : ArrayType(StringType())\n",
    "}\n",
    "estructurales.separarSeOrdena(columnas, spacy=False, actualizar_sdf=True)\n",
    "\n",
    "columnas = {\n",
    "    'extension sentencia' : IntegerType(),\n",
    "    'extension por lo tanto' : IntegerType()\n",
    "}\n",
    "estructurales.extraerExtension(columnas, True)\n",
    "\n",
    "#Sobreescribir el dataset de filtro de sentencias con las nuevas columnas\n",
    "estructurales.seleccion.guardarDatos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be285599-df3e-4e3e-9a91-8e9e594e6386",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Cargar procesamiento de sentencias estructurales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9faee8a1-7740-448a-9603-b5bc03ddedcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>archivo</th>\n",
       "      <th>txt</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>num</th>\n",
       "      <th>fechahora_ext</th>\n",
       "      <th>redactor_ext</th>\n",
       "      <th>tipoResolucion_ext</th>\n",
       "      <th>termino_ext</th>\n",
       "      <th>voto_salvado</th>\n",
       "      <th>tema</th>\n",
       "      <th>...</th>\n",
       "      <th>se_ordena</th>\n",
       "      <th>plan</th>\n",
       "      <th>plazo</th>\n",
       "      <th>se_ordena_PER</th>\n",
       "      <th>se_ordena_LOC</th>\n",
       "      <th>se_ordena_ORG</th>\n",
       "      <th>se_ordena_MISC</th>\n",
       "      <th>se_ordena_GPE</th>\n",
       "      <th>extension_sentencia</th>\n",
       "      <th>extension_por_lo_tanto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96AF8.html</td>\n",
       "      <td>\\n* 080147960007CO * Exp: 08-014796-0007-CO Re...</td>\n",
       "      <td>co exp co res sal constitucional cort suprem j...</td>\n",
       "      <td>132</td>\n",
       "      <td>2009-02-13 13:27:00</td>\n",
       "      <td>Abdelnour Granados</td>\n",
       "      <td>Recurso de Amparo</td>\n",
       "      <td>Con lugar</td>\n",
       "      <td>False</td>\n",
       "      <td>TRABAJO</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>19878</td>\n",
       "      <td>1641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96006.html</td>\n",
       "      <td>\\n* 090051320007CO * Exp: 09-005132-0007-CO Re...</td>\n",
       "      <td>co exp co res sal constitucional cort suprem j...</td>\n",
       "      <td>133</td>\n",
       "      <td>2009-04-30 18:33:00</td>\n",
       "      <td>Armijo Sancho</td>\n",
       "      <td>Recurso de Amparo</td>\n",
       "      <td>Con lugar</td>\n",
       "      <td>False</td>\n",
       "      <td>TRABAJO</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Ricardo Vindas Valerio]</td>\n",
       "      <td>None</td>\n",
       "      <td>[Director de   Recursos Humanos]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>12682</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9A831.html</td>\n",
       "      <td>\\n*090057240007CO* Exp: 09-005724-0007-CO Res....</td>\n",
       "      <td>co exp co res sal constitucional cort suprem j...</td>\n",
       "      <td>216696</td>\n",
       "      <td>NaT</td>\n",
       "      <td>Araya Garc√≠a</td>\n",
       "      <td>Recurso de Amparo</td>\n",
       "      <td>Con lugar parcial</td>\n",
       "      <td>False</td>\n",
       "      <td>PETICION</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[Miguel A. Carabagu√≠az Murillo, Carlos Fernand...</td>\n",
       "      <td>None</td>\n",
       "      <td>[Presidente Ejecutivo, Instituto Costarricense...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>25425</td>\n",
       "      <td>1917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9ED23.html</td>\n",
       "      <td>\\n*090035170007CO* Exp: 09-003517-0007-CO Res....</td>\n",
       "      <td>co exp co res sal constitucional cort suprem j...</td>\n",
       "      <td>216698</td>\n",
       "      <td>2009-05-08 11:29:00</td>\n",
       "      <td>None</td>\n",
       "      <td>Recurso de Amparo</td>\n",
       "      <td>Con lugar</td>\n",
       "      <td>False</td>\n",
       "      <td>TRABAJO</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Fernando Bogantes Cruz]</td>\n",
       "      <td>None</td>\n",
       "      <td>[Director de Recursos Humanos del Ministerio d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>7186</td>\n",
       "      <td>1672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94C54.html</td>\n",
       "      <td>\\n* 080177720007CO * Exp: 08-017772-0007-CO Re...</td>\n",
       "      <td>co exp co res sal constitucional cort suprem j...</td>\n",
       "      <td>216704</td>\n",
       "      <td>2009-03-17 11:36:00</td>\n",
       "      <td>Vargas Benavides</td>\n",
       "      <td>Recurso de Amparo</td>\n",
       "      <td>Con lugar</td>\n",
       "      <td>False</td>\n",
       "      <td>TRABAJO</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Fernando Bogantes Cruz, FRAN ARROYO PE√ëA]</td>\n",
       "      <td>[Zona de Menor Desarrollo]</td>\n",
       "      <td>[Director de Recursos Humanos del Ministerio d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>27198</td>\n",
       "      <td>1507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>95058.html</td>\n",
       "      <td>\\n* 090000300007CO * Exp: 09-000030-0007-CO Re...</td>\n",
       "      <td>co exp co res sal constitucional cort suprem j...</td>\n",
       "      <td>216716</td>\n",
       "      <td>2009-03-18 15:03:00</td>\n",
       "      <td>Vargas Benavides</td>\n",
       "      <td>Recurso de Amparo</td>\n",
       "      <td>Con lugar</td>\n",
       "      <td>False</td>\n",
       "      <td>TRABAJO</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Fernando Bogantes Cruz, ALVARADO D√çAZ]</td>\n",
       "      <td>[Zona de Menor Desarrollo]</td>\n",
       "      <td>[Director de Recursos Humanos del Ministerio d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>27096</td>\n",
       "      <td>1518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>93B9B.html</td>\n",
       "      <td>\\n* 080169410007CO * Exp: 08-016941-0007-CO Re...</td>\n",
       "      <td>co exp co res sal constitucional cort suprem j...</td>\n",
       "      <td>216717</td>\n",
       "      <td>2009-02-17 11:34:00</td>\n",
       "      <td>Calzada Miranda</td>\n",
       "      <td>Recurso de Amparo</td>\n",
       "      <td>Con lugar</td>\n",
       "      <td>False</td>\n",
       "      <td>TRABAJO</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Fernando Bogantes Cruz]</td>\n",
       "      <td>[Zona de Menor Desarrollo]</td>\n",
       "      <td>[Director de Recursos Humanos del Ministerio d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>27174</td>\n",
       "      <td>1516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9D741.html</td>\n",
       "      <td>\\n*080170600007CO* Exp: 08-017060-0007-CO Res....</td>\n",
       "      <td>co exp co res sal constitucional cort suprem j...</td>\n",
       "      <td>216719</td>\n",
       "      <td>2009-01-30 12:41:00</td>\n",
       "      <td>Cruz Castro</td>\n",
       "      <td>Recurso de Amparo</td>\n",
       "      <td>Con lugar</td>\n",
       "      <td>False</td>\n",
       "      <td>TRABAJO</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5794</td>\n",
       "      <td>1745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>941A8.html</td>\n",
       "      <td>\\n* 080169340007CO * Exp: 08-016934-0007-CO Re...</td>\n",
       "      <td>co exp co res sal constitucional cort suprem j...</td>\n",
       "      <td>216720</td>\n",
       "      <td>2009-02-17 10:02:00</td>\n",
       "      <td>Calzada Miranda</td>\n",
       "      <td>Recurso de Amparo</td>\n",
       "      <td>Con lugar</td>\n",
       "      <td>False</td>\n",
       "      <td>TRABAJO</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Fernando Bogantes Cruz, GEORGINA VARGAS PEREIRA]</td>\n",
       "      <td>[Zona de Menor Desarrollo]</td>\n",
       "      <td>[Director de Recursos Humanos del Ministerio d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>27167</td>\n",
       "      <td>1520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9979B.html</td>\n",
       "      <td>\\n*090053520007CO* EXPEDIENTE N¬∞ 09-005352-000...</td>\n",
       "      <td>co expedient co proces recurs ampar resolu sal...</td>\n",
       "      <td>216728</td>\n",
       "      <td>2009-07-21 19:24:00</td>\n",
       "      <td>Calzada Miranda</td>\n",
       "      <td>Recurso de Amparo</td>\n",
       "      <td>Con lugar parcial</td>\n",
       "      <td>False</td>\n",
       "      <td>TRABAJO</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Alberto Orozco Canossa, ERNESTO CHAVERRI VARGAS]</td>\n",
       "      <td>[Zona de Menor Desarrollo]</td>\n",
       "      <td>[Director de Recursos Humanos del Ministerio d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>26114</td>\n",
       "      <td>1609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      archivo                                                txt  \\\n",
       "0  96AF8.html  \\n* 080147960007CO * Exp: 08-014796-0007-CO Re...   \n",
       "1  96006.html  \\n* 090051320007CO * Exp: 09-005132-0007-CO Re...   \n",
       "2  9A831.html  \\n*090057240007CO* Exp: 09-005724-0007-CO Res....   \n",
       "3  9ED23.html  \\n*090035170007CO* Exp: 09-003517-0007-CO Res....   \n",
       "4  94C54.html  \\n* 080177720007CO * Exp: 08-017772-0007-CO Re...   \n",
       "5  95058.html  \\n* 090000300007CO * Exp: 09-000030-0007-CO Re...   \n",
       "6  93B9B.html  \\n* 080169410007CO * Exp: 08-016941-0007-CO Re...   \n",
       "7  9D741.html  \\n*080170600007CO* Exp: 08-017060-0007-CO Res....   \n",
       "8  941A8.html  \\n* 080169340007CO * Exp: 08-016934-0007-CO Re...   \n",
       "9  9979B.html  \\n*090053520007CO* EXPEDIENTE N¬∞ 09-005352-000...   \n",
       "\n",
       "                                           cleanText     num  \\\n",
       "0  co exp co res sal constitucional cort suprem j...     132   \n",
       "1  co exp co res sal constitucional cort suprem j...     133   \n",
       "2  co exp co res sal constitucional cort suprem j...  216696   \n",
       "3  co exp co res sal constitucional cort suprem j...  216698   \n",
       "4  co exp co res sal constitucional cort suprem j...  216704   \n",
       "5  co exp co res sal constitucional cort suprem j...  216716   \n",
       "6  co exp co res sal constitucional cort suprem j...  216717   \n",
       "7  co exp co res sal constitucional cort suprem j...  216719   \n",
       "8  co exp co res sal constitucional cort suprem j...  216720   \n",
       "9  co expedient co proces recurs ampar resolu sal...  216728   \n",
       "\n",
       "        fechahora_ext        redactor_ext tipoResolucion_ext  \\\n",
       "0 2009-02-13 13:27:00  Abdelnour Granados  Recurso de Amparo   \n",
       "1 2009-04-30 18:33:00       Armijo Sancho  Recurso de Amparo   \n",
       "2                 NaT        Araya Garc√≠a  Recurso de Amparo   \n",
       "3 2009-05-08 11:29:00                None  Recurso de Amparo   \n",
       "4 2009-03-17 11:36:00    Vargas Benavides  Recurso de Amparo   \n",
       "5 2009-03-18 15:03:00    Vargas Benavides  Recurso de Amparo   \n",
       "6 2009-02-17 11:34:00     Calzada Miranda  Recurso de Amparo   \n",
       "7 2009-01-30 12:41:00         Cruz Castro  Recurso de Amparo   \n",
       "8 2009-02-17 10:02:00     Calzada Miranda  Recurso de Amparo   \n",
       "9 2009-07-21 19:24:00     Calzada Miranda  Recurso de Amparo   \n",
       "\n",
       "         termino_ext  voto_salvado      tema  ... se_ordena plan plazo  \\\n",
       "0          Con lugar         False   TRABAJO  ...         1    0     0   \n",
       "1          Con lugar         False   TRABAJO  ...         1    0     0   \n",
       "2  Con lugar parcial         False  PETICION  ...         1    0     1   \n",
       "3          Con lugar         False   TRABAJO  ...         1    0     0   \n",
       "4          Con lugar         False   TRABAJO  ...         1    0     0   \n",
       "5          Con lugar         False   TRABAJO  ...         1    0     0   \n",
       "6          Con lugar         False   TRABAJO  ...         1    0     0   \n",
       "7          Con lugar         False   TRABAJO  ...         0    0     1   \n",
       "8          Con lugar         False   TRABAJO  ...         1    0     0   \n",
       "9  Con lugar parcial         False   TRABAJO  ...         1    0     0   \n",
       "\n",
       "                                       se_ordena_PER  \\\n",
       "0                                               None   \n",
       "1                           [Ricardo Vindas Valerio]   \n",
       "2  [Miguel A. Carabagu√≠az Murillo, Carlos Fernand...   \n",
       "3                           [Fernando Bogantes Cruz]   \n",
       "4         [Fernando Bogantes Cruz, FRAN ARROYO PE√ëA]   \n",
       "5            [Fernando Bogantes Cruz, ALVARADO D√çAZ]   \n",
       "6                           [Fernando Bogantes Cruz]   \n",
       "7                                               None   \n",
       "8  [Fernando Bogantes Cruz, GEORGINA VARGAS PEREIRA]   \n",
       "9  [Alberto Orozco Canossa, ERNESTO CHAVERRI VARGAS]   \n",
       "\n",
       "                se_ordena_LOC  \\\n",
       "0                        None   \n",
       "1                        None   \n",
       "2                        None   \n",
       "3                        None   \n",
       "4  [Zona de Menor Desarrollo]   \n",
       "5  [Zona de Menor Desarrollo]   \n",
       "6  [Zona de Menor Desarrollo]   \n",
       "7                        None   \n",
       "8  [Zona de Menor Desarrollo]   \n",
       "9  [Zona de Menor Desarrollo]   \n",
       "\n",
       "                                       se_ordena_ORG se_ordena_MISC  \\\n",
       "0                                               None           None   \n",
       "1                   [Director de   Recursos Humanos]           None   \n",
       "2  [Presidente Ejecutivo, Instituto Costarricense...           None   \n",
       "3  [Director de Recursos Humanos del Ministerio d...           None   \n",
       "4  [Director de Recursos Humanos del Ministerio d...           None   \n",
       "5  [Director de Recursos Humanos del Ministerio d...           None   \n",
       "6  [Director de Recursos Humanos del Ministerio d...           None   \n",
       "7                                               None           None   \n",
       "8  [Director de Recursos Humanos del Ministerio d...           None   \n",
       "9  [Director de Recursos Humanos del Ministerio d...           None   \n",
       "\n",
       "   se_ordena_GPE extension_sentencia extension_por_lo_tanto  \n",
       "0           None               19878                   1641  \n",
       "1           None               12682                   1597  \n",
       "2           None               25425                   1917  \n",
       "3           None                7186                   1672  \n",
       "4           None               27198                   1507  \n",
       "5           None               27096                   1518  \n",
       "6           None               27174                   1516  \n",
       "7           None                5794                   1745  \n",
       "8           None               27167                   1520  \n",
       "9           None               26114                   1609  \n",
       "\n",
       "[10 rows x 38 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminos = {\n",
    "    'seguimiento': [r'\\bseguimiento\\b'],\n",
    "    'se ordena': [r'\\bse ordena\\b'],\n",
    "    'plan': [r'\\bplan\\b'],\n",
    "    'plazo': [r'\\bplazo\\b']\n",
    "}\n",
    "seleccion = Seleccion(terminos, spark, parquet_path='../../src/datasets/complete', datasets_path='./datasets/')\n",
    "seleccion.cargarPreprocesados()\n",
    "\n",
    "estructurales = SentenciasEstructurales(seleccion)\n",
    "estructurales.seleccion.sdf.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f70af-a6a7-47cf-9579-a56a1ab2b054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estructurales.seleccion.sdf.toPandas()[estructurales.seleccion.sdf.toPandas()['plazo'] > 0]['txt'].iloc[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a08142a-3d1a-4dfe-a02e-12fdc211a8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(estructurales.seleccion.sdf.filter(\"expediente == '170020150007CO'\").select(\"txt\").toPandas().loc[0,'txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be45513-1ddd-4b10-aaf7-40d1959f364d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "txt = \"\"\" Se declara con lugar el recurso. Se ordena a Yaxinia D√≠az Mendoza, en calidad de Directora de Recursos Humanos, a Rosa Adolio Cascante, en calidad de Viceministra Administrativa y a Walter Mu√±oz Caravaca en su calidad de Director de Infraestructura y Equipamiento, todos del Ministerio de Educaci√≥n P√∫blica o a quienes en su lugar ocupen los cargos, cada uno dentro del √°mbito de sus competencias, que en el plazo m√°ximo de OCHO D√çAS contados a partir de la notificaci√≥n de esta sentencia realicen el tr√°mite correspondiente para nombrar un Profesor de Ense√±anza General B√°sica 1 en la Escuela La Katira. Deber√°n adem√°s en el plazo m√°ximo de TRES MESES contados a partir de la notificaci√≥n de esta sentencia iniciar con los procedimientos necesarios ‚Äìprocedimientos que incluyen tanto la aprobaci√≥n de presupuesto como el proceso de contrataci√≥n- para la construcci√≥n de la infraestructura requerida por la Escuela La Katira. En ambas √≥rdenes se debe garantizar a los estudiantes perjudicados su continuidad en el proceso educativo. Se advierte a dichos funcionarios que de no acatar la orden dicha, incurrir√°n en el delito de desobediencia y, que de conformidad con el art√≠culo 71 de la Ley de esta jurisdicci√≥n, se le impondr√° prisi√≥n de tres meses a dos a√±os, o de veinte a sesenta d√≠as multa, a quien recibiere una orden que deba cumplir o hacer cumplir, dictada en un recurso de amparo y no la cumpliere o no la hiciere cumplir, siempre que el delito no est√© m√°s gravemente penado. Se condena al Estado al pago de las costas, da√±os y perjuicios causados con los hechos que sirven de base a esta declaratoria, los que ser√°n liquidados en ejecuci√≥n de sentencia de lo contencioso administrativo. El Magistrado Salazar Alvarado pone nota. La Magistrada Esquivel Rodr√≠guez consigna nota. Notif√≠quese la presente resoluci√≥n a Yaxinia D√≠az Mendoza, en calidad de Directora de Recursos Humanos, a Rosa Adolio Cascante, en calidad de Viceministra Administrativa y a Walter Mu√±oz Caravaca en su calidad de Director de Infraestructura y Equipamiento, todos del Ministerio de Educaci√≥n P√∫blica o a quienes en su lugar ocupen los cargos, EN FORMA PERSONAL. Deber√°n las autoridades recurridas informar a esta Sala sobre el avance en la construcci√≥n de las aulas en la Escuela La Katira. Comun√≠quese al Ministro de Educaci√≥n P√∫blica el contenido de esta sentencia.\n",
    "\"\"\"\n",
    "patron = [{\"LOWER\": \"plazo\"},\n",
    "          {\"TEXT\": {\"REGEX\": \"^(?!\\.|[Hh][Oo√ì√≥][Rr][Aa]([Ss])?|[dD][i√≠I√ç][√ÅAaa]([Ss])?|[Mm][Ee√â√©][Ss]([Ee√â√©][Ss])?|[√Å√°Aa][√ë√±][Oo√ì√≥]([Ss])?)\"}, \"OP\": \"+\"},\n",
    "          {\"TEXT\": {\"REGEX\": \"\\.|[Hh][Oo√ì√≥][Rr][Aa]([Ss])?|[dD][i√≠I√ç][√ÅAaa]([Ss])?|[Mm][Ee√â√©][Ss]([Ee√â√©][Ss])?|[√Å√°Aa][√ë√±][Oo√ì√≥]([Ss])?\"}},\n",
    "          ]\n",
    "\n",
    "patterns = [patron]\n",
    "\n",
    "nlp = spark_get_spacy('es_core_news_lg')\n",
    "doc = nlp(txt)\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"Patron 1 :\", patterns, greedy=\"FIRST\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "plazos = []\n",
    "for match_id, start, end in matches:\n",
    "    includeText = False\n",
    "    plazo = \"\"\n",
    "    for token in doc[start:end]:\n",
    "        if includeText:\n",
    "            if token.pos_ == \"PUNCT\":\n",
    "                break\n",
    "            plazo += \" \" + token.text\n",
    "        else:\n",
    "            if token.pos_ == \"NUM\":\n",
    "                plazo += token.text\n",
    "                includeText = True\n",
    "    if plazo != \"\":\n",
    "        plazos.append(plazo)\n",
    "                \n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    print(\"SPAN\")\n",
    "    span =  doc[start:end]\n",
    "    print(span.text)\n",
    "\n",
    "print(\"PLAZOS\")\n",
    "print(plazos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a5e74-3c2f-4bb7-935a-dccb2dd2cf6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estructurales.sdf.toPandas()[estructurales.sdf.toPandas()['expediente'] == '040099160007CO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4cf074-f04d-4972-a2f5-166bd6891c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "columnas = {\n",
    "    'plazosDefinidos' : ArrayType(StringType())\n",
    "}\n",
    "resultado = estructurales.plazosDefinidos(columnas)\n",
    "resultado.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e36ae8-4f0d-4a14-b376-1907070f6715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultado.toPandas()[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb5138-3b87-4e02-9461-1a82ce6cea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plazo = \"\"\n",
    "plazo = plazo + \" perro \"\n",
    "plazo = plazo + \" gato \"\n",
    "print(plazo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b741c960-0ca9-42bd-8656-a17407c2a4fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(pd.Timedelta('2 month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac894766-02ef-42b3-b54d-87256b21a895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = \"9:05\"\n",
    "print(t.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e684bad8-54d8-4067-be3f-1d5c9bc7e0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
