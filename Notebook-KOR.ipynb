{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f56ce3-6a94-4fc2-a051-945ebfb32480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a834852-b9fd-483c-93e6-3b80f3e879c5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2ec801-fdee-413e-aca9-8ee8e77e0cac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from nlppen.extraccion.utils.Txt2Numbers import Txt2Numbers\n",
    "from nlppen.analisis import Analisis\n",
    "from nlppen.seleccion import Seleccion\n",
    "from nlppen.spark_udfs import solo_portanto, spark_get_spacy\n",
    "from nlppen.sentencias_estructurales import SentenciasEstructurales\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106f23a-2bb1-483d-bc31-aa48b9a7db2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialize spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b819481b-6ee8-42c7-9add-c19ceba43563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/30 15:16:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://69abb479a5e1:4040'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Transforming sentences\")\n",
    "         .config(\"spark.num.executors\", \"1\")\n",
    "         .config(\"spark.executor.memory\", \"12g\")\n",
    "         .config(\"spark.executor.cores\", \"2\")\n",
    "         .config(\"spark.driver.memory\", \"10g\")\n",
    "         .config(\"spark.memory.offHeap.enabled\", True)\n",
    "         .config(\"spark.memory.offHeap.size\", \"64g\")\n",
    "         .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "         .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "         .config(\"spark.sql.legacy.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "         .getOrCreate())\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.addFile('/home/jovyan/Work/ej/paquetes/nlppen/nlppen.zip')\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab75fdc-6e47-4671-b06b-4109f7a2bc66",
   "metadata": {},
   "source": [
    "# Buscar terminos en la sección de por lo tanto de la sentencia y aplicar filtro del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a92cddd-4b24-4bdf-85d7-ceabe68d009d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "507951"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "anno_inf = 2010\n",
    "anno_sup = 2022\n",
    "\n",
    "terminos = {\n",
    "    'se ordena': [r'\\bse ordena\\b', r'\\bse le ordena\\b', r'\\bse les ordena\\b'],\n",
    "    'se condena': [r'\\bse condena\\b', r'\\bse le condena\\b', r'\\bse les condena\\b'],\n",
    "    'plan': [r'\\bplan\\b']\n",
    "}\n",
    "\n",
    "# seleccion = Seleccion(terminos, spark, parquet_path='../../src/datasets/complete', datasets_path='./datasets/estructurales2', filtro=f'anno >= {anno_inf} AND anno < {anno_sup}')\n",
    "seleccion = Seleccion(terminos, spark, parquet_path='./datasets/estructurales2/extracciones2', datasets_path='./datasets/estructurales3')\n",
    "# seleccion.filtrar_sentencias(preprocess=solo_portanto, keepRowEmpty=True, partition_cols=['anno'], precargar=False)\n",
    "\n",
    "estructurales = SentenciasEstructurales(seleccion)\n",
    "\n",
    "seleccion.sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b108727-4f70-4ec4-b389-4eb10908e167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96773"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('../../src/datasets/complete').filter('termino_ext == \"Con lugar\" OR  termino_ext == \"Con lugar parcial\" ').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2283e2de-42e6-4fd5-84ba-031847b86f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['archivo',\n",
       " 'txt',\n",
       " 'cleanText',\n",
       " 'num',\n",
       " 'fechahora_ext',\n",
       " 'redactor_ext',\n",
       " 'tipoResolucion_ext',\n",
       " 'termino_ext',\n",
       " 'voto_salvado',\n",
       " 'tema',\n",
       " 'Recurrente_ents',\n",
       " 'Cantón_ents',\n",
       " 'Fecha_ents',\n",
       " 'Constitución_ents',\n",
       " 'Redactor_ents',\n",
       " 'Ley_ents',\n",
       " 'EntidadPublica_ents',\n",
       " 'tema_prob',\n",
       " 'termino_lst',\n",
       " 'expediente',\n",
       " 'Fechahora_ents',\n",
       " 'CitaSentencias_ents',\n",
       " 'Recurrido_ents',\n",
       " 'Reglamento_ents',\n",
       " 'Magistrado_ents',\n",
       " '_id',\n",
       " 'seguimiento',\n",
       " 'se_ordena',\n",
       " 'se_condena',\n",
       " 'plan',\n",
       " 'plazo',\n",
       " 'se_ordena_PER',\n",
       " 'se_ordena_LOC',\n",
       " 'se_ordena_ORG',\n",
       " 'se_ordena_MISC',\n",
       " 'se_ordena_GPE',\n",
       " 'se_ordena_Ent_Pub',\n",
       " 'se_condena_PER',\n",
       " 'se_condena_LOC',\n",
       " 'se_condena_ORG',\n",
       " 'se_condena_MISC',\n",
       " 'se_condena_GPE',\n",
       " 'se_condena_Ent_Pub',\n",
       " 'extension_sentencia',\n",
       " 'extension_por_lo_tanto',\n",
       " 'plazosDefinidos',\n",
       " 'FechaSolicitud',\n",
       " 'num_resolucion',\n",
       " 'inst_internacionales',\n",
       " 'derechos_Norm',\n",
       " 'derechos_GenXPat',\n",
       " 'derechos_Acotados',\n",
       " 'derechos_General',\n",
       " 'derechos_Fundamental',\n",
       " 'derechos_Humano',\n",
       " 'citasFechas',\n",
       " 'id_sentencia',\n",
       " 'num_doc_oficial',\n",
       " 'expediente_oficial',\n",
       " 'anno']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seleccion.sdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b9686-78ed-4c53-9cf9-694c588b45e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Formar dataset de sentencias estructurales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04f4588f-3661-4f06-bcdd-d5b1720c3e92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/24 16:05:33 WARN DAGScheduler: Broadcasting large task binary with size 1151.1 KiB\n",
      "21/11/24 16:22:02 WARN MemoryStore: Not enough space to cache rdd_58_10 in memory! (computed 263.5 MiB so far)\n",
      "21/11/24 16:22:02 WARN BlockManager: Persisting block rdd_58_10 to disk instead.\n",
      "21/11/24 16:54:59 WARN MemoryStore: Not enough space to cache rdd_97_6 in memory! (computed 264.1 MiB so far)\n",
      "21/11/24 16:54:59 WARN BlockManager: Persisting block rdd_97_6 to disk instead.\n",
      "21/11/24 21:35:46 WARN DAGScheduler: Broadcasting large task binary with size 1329.3 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "columnas = {\n",
    "    'se ordena PER' : ArrayType(StringType()),\n",
    "    'se ordena LOC' : ArrayType(StringType()),\n",
    "    'se ordena ORG' : ArrayType(StringType()),\n",
    "    'se ordena MISC' : ArrayType(StringType()),\n",
    "    'se ordena GPE' : ArrayType(StringType()),\n",
    "    'se ordena Ent Pub' : ArrayType(StringType())\n",
    "}\n",
    "estructurales.separarSeOrdena(columnas, True, True)\n",
    "\n",
    "columnas = {\n",
    "    'se condena PER' : ArrayType(StringType()),\n",
    "    'se condena LOC' : ArrayType(StringType()),\n",
    "    'se condena ORG' : ArrayType(StringType()),\n",
    "    'se condena MISC' : ArrayType(StringType()),\n",
    "    'se condena GPE' : ArrayType(StringType()),\n",
    "    'se condena Ent Pub' : ArrayType(StringType())\n",
    "}\n",
    "estructurales.separarSeCondena(columnas, True, True)\n",
    "\n",
    "columnas = {\n",
    "    'extension sentencia' : IntegerType(),\n",
    "    'extension por lo tanto' : IntegerType()\n",
    "}\n",
    "estructurales.extraerExtension(columnas, True)\n",
    "\n",
    "columnas = {\n",
    "        'plazosDefinidos' : ArrayType(TimestampType())\n",
    "    }\n",
    "estructurales.plazosDefinidos(columnas, True)\n",
    "\n",
    "columnas = {\n",
    "    'FechaSolicitud' : TimestampType(),\n",
    "}\n",
    "\n",
    "estructurales.extrarFechaRecibido(columnas, True)\n",
    "\n",
    "columnas = {\n",
    "    'num resolucion' : StringType()\n",
    "}\n",
    "\n",
    "estructurales.extraerNumeroSentencia(columnas, True)\n",
    "\n",
    "columnas = {\n",
    "    'inst internacionales' : ArrayType(StringType())\n",
    "}\n",
    "\n",
    "estructurales.extraerInstrumentosInternacionales(columnas, True)\n",
    "\n",
    "columnas = {\n",
    "    'derechos Norm' : ArrayType(StringType()),\n",
    "    'derechos GenXPat' : ArrayType(StringType())\n",
    "}\n",
    "estructurales.extraerDerechos(columnas, True)\n",
    "\n",
    "columnas = {\n",
    "    'derechos Acotados' : ArrayType(StringType()),\n",
    "    'derechos General' : ArrayType(StringType()),\n",
    "    'derechos Fundamental' : ArrayType(StringType()),\n",
    "    'derechos Humano' : ArrayType(StringType())\n",
    "}\n",
    "\n",
    "estructurales.extraerDerechosSinNormalizar(columnas, True)\n",
    "\n",
    "columnas = {\n",
    "    'citasFechas' : MapType(StringType(), TimestampType())\n",
    "}\n",
    "estructurales.extrarCitaSentenciasFecha(columnas, True)\n",
    "\n",
    "sentencias = pd.read_csv(\"listaSentencias.csv\", sep=';', encoding='latin-1')\n",
    "columnas = {\n",
    "    'id_sentencia' : StringType(),\n",
    "    'num_doc_oficial' : StringType(),\n",
    "    'expediente_oficial' : StringType()\n",
    "}\n",
    "estructurales.agregarIDSentencia(columnas, sentencias, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162175ab-56bb-46f0-a87a-dd98911a34ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[archivo: string, txt: string, cleanText: string, num: bigint, fechahora_ext: timestamp, redactor_ext: string, tipoResolucion_ext: string, termino_ext: string, voto_salvado: boolean, tema: string, Recurrente_ents: array<string>, Cantón_ents: array<string>, Fecha_ents: array<string>, Constitución_ents: array<bigint>, Redactor_ents: array<string>, Ley_ents: array<string>, EntidadPublica_ents: array<string>, tema_prob: double, termino_lst: array<string>, expediente: string, Fechahora_ents: array<string>, CitaSentencias_ents: array<string>, Recurrido_ents: array<string>, Reglamento_ents: array<string>, Magistrado_ents: array<string>, _id: string, seguimiento: int, se_ordena: int, se_condena: int, plan: int, plazo: int, se_ordena_PER: array<string>, se_ordena_LOC: array<string>, se_ordena_ORG: array<string>, se_ordena_MISC: array<string>, se_ordena_GPE: array<string>, se_ordena_Ent_Pub: array<string>, se_condena_PER: array<string>, se_condena_LOC: array<string>, se_condena_ORG: array<string>, se_condena_MISC: array<string>, se_condena_GPE: array<string>, se_condena_Ent_Pub: array<string>, extension_sentencia: int, extension_por_lo_tanto: int, plazosDefinidos: array<timestamp>, FechaSolicitud: timestamp, num_resolucion: string, inst_internacionales: array<string>, derechos_Norm: array<string>, derechos_GenXPat: array<string>, derechos_Acotados: array<string>, derechos_General: array<string>, derechos_Fundamental: array<string>, derechos_Humano: array<string>, citasFechas: map<string,timestamp>, id_sentencia: string, num_doc_oficial: string, expediente_oficial: string, anno: int, citasVotDate: map<string,timestamp>, citasIDVoto: map<string,string>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentencias = pd.read_csv(\"listaSentencias.csv\", sep=';', encoding='latin-1')\n",
    "columnas = {\n",
    "        'citasVotDate' : MapType(StringType(), TimestampType()),\n",
    "        'citasIDVoto' : MapType(StringType(), StringType())\n",
    "    }\n",
    "estructurales.extrarCitaSentenciasFecha(columnas, sentencias, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1980ab-4927-40aa-abb2-7ec76b0e20c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3256/2419603567.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m'seguimientoValue'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Cantidad de ocurrencias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     }\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mestructurales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextraerSeguimiento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumnas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentencias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\u001b[0m in \u001b[0;36mextraerSeguimiento\u001b[0;34m(self, addColumns, actualizar_sdf, overwriteColumns)\u001b[0m\n\u001b[1;32m    344\u001b[0m                     )\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mactualizar_sdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseleccion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresultado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1538\u001b[0m             \u001b[0;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m             \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "#Palabras de seeguimiento de acuerdo al correo de Eve.\n",
    "columnas = {\n",
    "        'seguimientoExt' : StringType(), #Texto extraido de seguimiento\n",
    "        'seguimientoValue' : IntegerType() #Cantidad de ocurrencias\n",
    "    }\n",
    "estructurales.extraerSeguimiento(columnas, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576c47b4-c583-444c-a90e-0475532a1b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/30 15:16:44 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[archivo: string, txt: string, cleanText: string, num: bigint, fechahora_ext: timestamp, redactor_ext: string, tipoResolucion_ext: string, termino_ext: string, voto_salvado: boolean, tema: string, Recurrente_ents: array<string>, Cantón_ents: array<string>, Fecha_ents: array<string>, Constitución_ents: array<bigint>, Redactor_ents: array<string>, Ley_ents: array<string>, EntidadPublica_ents: array<string>, tema_prob: double, termino_lst: array<string>, expediente: string, Fechahora_ents: array<string>, CitaSentencias_ents: array<string>, Recurrido_ents: array<string>, Reglamento_ents: array<string>, Magistrado_ents: array<string>, _id: string, seguimiento: int, se_ordena: int, se_condena: int, plan: int, plazo: int, se_ordena_PER: array<string>, se_ordena_LOC: array<string>, se_ordena_ORG: array<string>, se_ordena_MISC: array<string>, se_ordena_GPE: array<string>, se_ordena_Ent_Pub: array<string>, se_condena_PER: array<string>, se_condena_LOC: array<string>, se_condena_ORG: array<string>, se_condena_MISC: array<string>, se_condena_GPE: array<string>, se_condena_Ent_Pub: array<string>, extension_sentencia: int, extension_por_lo_tanto: int, plazosDefinidos: array<timestamp>, FechaSolicitud: timestamp, num_resolucion: string, inst_internacionales: array<string>, derechos_Norm: array<string>, derechos_GenXPat: array<string>, derechos_Acotados: array<string>, derechos_General: array<string>, derechos_Fundamental: array<string>, derechos_Humano: array<string>, citasFechas: map<string,timestamp>, id_sentencia: string, num_doc_oficial: string, expediente_oficial: string, anno: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "#NOTA IMPORTANTE PARA KENNETH\n",
    "#Plazos hay que correrlo de nuevo, las lines de abajo sobreescriben la columna 'plazosDefinidos' que se corrió previamente\n",
    "columnas = {\n",
    "        'plazosDefinidos' : ArrayType(TimestampType()) #Correción día 27-11-2021. Correr de nuevo\n",
    "    }\n",
    "estructurales.plazosDefinidos(columnas, actualizar_sdf=True, overwriteColumns=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "997e42d6-6815-4ebb-ab20-de50bd08ce4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/30 15:41:17 WARN MemoryStore: Not enough space to cache rdd_20_5 in memory! (computed 68.8 MiB so far)\n",
      "21/11/30 15:41:17 WARN MemoryStore: Not enough space to cache rdd_20_8 in memory! (computed 69.0 MiB so far)\n",
      "21/11/30 15:41:17 WARN MemoryStore: Not enough space to cache rdd_20_7 in memory! (computed 69.0 MiB so far)\n",
      "21/11/30 15:41:17 WARN MemoryStore: Not enough space to cache rdd_20_14 in memory! (computed 68.6 MiB so far)\n",
      "21/11/30 15:41:17 WARN MemoryStore: Not enough space to cache rdd_20_3 in memory! (computed 134.9 MiB so far)\n",
      "21/11/30 15:41:18 WARN MemoryStore: Not enough space to cache rdd_20_6 in memory! (computed 263.4 MiB so far)\n",
      "21/11/30 15:59:04 WARN BlockManager: Putting block rdd_49_2 failed due to exception org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 168, in <lambda>\n",
      "    .map( lambda row : spark_extraer_fecha_cita_sentencia(row, newColumns, datasetSentencias, None))\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 280, in spark_extraer_fecha_cita_sentencia\n",
      "    sentenciasConFecha , sentenciasCitadas  = extraerCitaSentencia.extraerCitas(txt, datasetSentencias)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 173, in extraerCitas\n",
      "    sentenciasCitadasID = self.__extraerID(sentenciasCitadas, sentenciasCSV)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 90, in __extraerID\n",
      "    sentenciaId, numVoto, _ = splitResolucion(cita, None, sentenciasCSV, citas[cita])\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/misc.py\", line 65, in splitResolucion\n",
      "    if exp.match(splitNumRes[1]):\n",
      "IndexError: list index out of range\n",
      ".\n",
      "21/11/30 15:59:04 WARN BlockManager: Block rdd_49_2 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:04 ERROR Executor: Exception in task 2.0 in stage 7.0 (TID 307)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 168, in <lambda>\n",
      "    .map( lambda row : spark_extraer_fecha_cita_sentencia(row, newColumns, datasetSentencias, None))\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 280, in spark_extraer_fecha_cita_sentencia\n",
      "    sentenciasConFecha , sentenciasCitadas  = extraerCitaSentencia.extraerCitas(txt, datasetSentencias)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 173, in extraerCitas\n",
      "    sentenciasCitadasID = self.__extraerID(sentenciasCitadas, sentenciasCSV)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 90, in __extraerID\n",
      "    sentenciaId, numVoto, _ = splitResolucion(cita, None, sentenciasCSV, citas[cita])\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/misc.py\", line 65, in splitResolucion\n",
      "    if exp.match(splitNumRes[1]):\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/11/30 15:59:04 WARN TaskSetManager: Lost task 2.0 in stage 7.0 (TID 307) (69abb479a5e1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 168, in <lambda>\n",
      "    .map( lambda row : spark_extraer_fecha_cita_sentencia(row, newColumns, datasetSentencias, None))\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 280, in spark_extraer_fecha_cita_sentencia\n",
      "    sentenciasConFecha , sentenciasCitadas  = extraerCitaSentencia.extraerCitas(txt, datasetSentencias)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 173, in extraerCitas\n",
      "    sentenciasCitadasID = self.__extraerID(sentenciasCitadas, sentenciasCSV)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 90, in __extraerID\n",
      "    sentenciaId, numVoto, _ = splitResolucion(cita, None, sentenciasCSV, citas[cita])\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/misc.py\", line 65, in splitResolucion\n",
      "    if exp.match(splitNumRes[1]):\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/11/30 15:59:04 ERROR TaskSetManager: Task 2 in stage 7.0 failed 1 times; aborting job\n",
      "21/11/30 15:59:04 ERROR FileFormatWriter: Aborting job 82bf1afb-882e-46e8-8062-8b875bf50ff4.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 7.0 failed 1 times, most recent failure: Lost task 2.0 in stage 7.0 (TID 307) (69abb479a5e1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 168, in <lambda>\n",
      "    .map( lambda row : spark_extraer_fecha_cita_sentencia(row, newColumns, datasetSentencias, None))\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 280, in spark_extraer_fecha_cita_sentencia\n",
      "    sentenciasConFecha , sentenciasCitadas  = extraerCitaSentencia.extraerCitas(txt, datasetSentencias)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 173, in extraerCitas\n",
      "    sentenciasCitadasID = self.__extraerID(sentenciasCitadas, sentenciasCSV)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 90, in __extraerID\n",
      "    sentenciaId, numVoto, _ = splitResolucion(cita, None, sentenciasCSV, citas[cita])\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/misc.py\", line 65, in splitResolucion\n",
      "    if exp.match(splitNumRes[1]):\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 168, in <lambda>\n",
      "    .map( lambda row : spark_extraer_fecha_cita_sentencia(row, newColumns, datasetSentencias, None))\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 280, in spark_extraer_fecha_cita_sentencia\n",
      "    sentenciasConFecha , sentenciasCitadas  = extraerCitaSentencia.extraerCitas(txt, datasetSentencias)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 173, in extraerCitas\n",
      "    sentenciasCitadasID = self.__extraerID(sentenciasCitadas, sentenciasCSV)\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 90, in __extraerID\n",
      "    sentenciaId, numVoto, _ = splitResolucion(cita, None, sentenciasCSV, citas[cita])\n",
      "  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/misc.py\", line 65, in splitResolucion\n",
      "    if exp.match(splitNumRes[1]):\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "21/11/30 15:59:04 WARN BlockManager: Putting block rdd_49_21 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:04 WARN BlockManager: Block rdd_49_21 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:04 WARN TaskSetManager: Lost task 21.0 in stage 7.0 (TID 326) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "[Stage 7:>                                                        (0 + 23) / 34]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o145.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 7.0 failed 1 times, most recent failure: Lost task 2.0 in stage 7.0 (TID 307) (69abb479a5e1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 168, in <lambda>\n    .map( lambda row : spark_extraer_fecha_cita_sentencia(row, newColumns, datasetSentencias, None))\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 280, in spark_extraer_fecha_cita_sentencia\n    sentenciasConFecha , sentenciasCitadas  = extraerCitaSentencia.extraerCitas(txt, datasetSentencias)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 173, in extraerCitas\n    sentenciasCitadasID = self.__extraerID(sentenciasCitadas, sentenciasCSV)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 90, in __extraerID\n    sentenciaId, numVoto, _ = splitResolucion(cita, None, sentenciasCSV, citas[cita])\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/misc.py\", line 65, in splitResolucion\n    if exp.match(splitNumRes[1]):\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\t... 33 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 168, in <lambda>\n    .map( lambda row : spark_extraer_fecha_cita_sentencia(row, newColumns, datasetSentencias, None))\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 280, in spark_extraer_fecha_cita_sentencia\n    sentenciasConFecha , sentenciasCitadas  = extraerCitaSentencia.extraerCitas(txt, datasetSentencias)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 173, in extraerCitas\n    sentenciasCitadasID = self.__extraerID(sentenciasCitadas, sentenciasCSV)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 90, in __extraerID\n    sentenciaId, numVoto, _ = splitResolucion(cita, None, sentenciasCSV, citas[cita])\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/misc.py\", line 65, in splitResolucion\n    if exp.match(splitNumRes[1]):\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3256/7020622.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Sobreescribir el dataset de filtro de sentencias con las nuevas columnas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mestructurales\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseleccion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguardarDatos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'extracciones3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anno'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mborrar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Work/ej/paquetes/nlppen/nlppen/seleccion.py\u001b[0m in \u001b[0;36mguardarDatos\u001b[0;34m(self, parquet_file, partition_cols, borrar)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 (self.sdf.repartition(*partition_cols)\n\u001b[0m\u001b[1;32m     80\u001b[0m                 .write.mode('append').partitionBy(*partition_cols).parquet(parquet_path))\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o145.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 7.0 failed 1 times, most recent failure: Lost task 2.0 in stage 7.0 (TID 307) (69abb479a5e1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 168, in <lambda>\n    .map( lambda row : spark_extraer_fecha_cita_sentencia(row, newColumns, datasetSentencias, None))\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 280, in spark_extraer_fecha_cita_sentencia\n    sentenciasConFecha , sentenciasCitadas  = extraerCitaSentencia.extraerCitas(txt, datasetSentencias)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 173, in extraerCitas\n    sentenciasCitadasID = self.__extraerID(sentenciasCitadas, sentenciasCSV)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 90, in __extraerID\n    sentenciaId, numVoto, _ = splitResolucion(cita, None, sentenciasCSV, citas[cita])\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/misc.py\", line 65, in splitResolucion\n    if exp.match(splitNumRes[1]):\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\t... 33 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/jovyan/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/sentencias_estructurales.py\", line 168, in <lambda>\n    .map( lambda row : spark_extraer_fecha_cita_sentencia(row, newColumns, datasetSentencias, None))\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/spark_udfs.py\", line 280, in spark_extraer_fecha_cita_sentencia\n    sentenciasConFecha , sentenciasCitadas  = extraerCitaSentencia.extraerCitas(txt, datasetSentencias)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 173, in extraerCitas\n    sentenciasCitadasID = self.__extraerID(sentenciasCitadas, sentenciasCSV)\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/extraerFechaCitaSentencia.py\", line 90, in __extraerID\n    sentenciaId, numVoto, _ = splitResolucion(cita, None, sentenciasCSV, citas[cita])\n  File \"/home/jovyan/Work/ej/paquetes/nlppen/nlppen/extraccion/utils/misc.py\", line 65, in splitResolucion\n    if exp.match(splitNumRes[1]):\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:222)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:335)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/30 15:59:05 WARN BlockManager: Putting block rdd_49_24 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:05 WARN BlockManager: Block rdd_49_24 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:05 WARN TaskSetManager: Lost task 24.0 in stage 7.0 (TID 329) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:05 WARN BlockManager: Putting block rdd_49_17 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:05 WARN BlockManager: Block rdd_49_17 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:05 WARN TaskSetManager: Lost task 17.0 in stage 7.0 (TID 322) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:06 WARN BlockManager: Putting block rdd_49_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:06 WARN BlockManager: Block rdd_49_3 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:06 WARN TaskSetManager: Lost task 3.0 in stage 7.0 (TID 308) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:06 WARN BlockManager: Putting block rdd_49_12 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:06 WARN BlockManager: Block rdd_49_12 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:06 WARN TaskSetManager: Lost task 12.0 in stage 7.0 (TID 317) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:06 WARN BlockManager: Putting block rdd_49_15 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:06 WARN BlockManager: Block rdd_49_15 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:06 WARN TaskSetManager: Lost task 15.0 in stage 7.0 (TID 320) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:07 WARN BlockManager: Putting block rdd_49_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:07 WARN BlockManager: Block rdd_49_1 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:07 WARN TaskSetManager: Lost task 1.0 in stage 7.0 (TID 306) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:07 WARN BlockManager: Putting block rdd_49_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:07 WARN BlockManager: Block rdd_49_8 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:07 WARN TaskSetManager: Lost task 8.0 in stage 7.0 (TID 313) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:07 WARN BlockManager: Putting block rdd_49_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:07 WARN BlockManager: Block rdd_49_7 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:07 WARN TaskSetManager: Lost task 7.0 in stage 7.0 (TID 312) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:07 WARN BlockManager: Putting block rdd_49_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:07 WARN BlockManager: Block rdd_49_6 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:07 WARN TaskSetManager: Lost task 6.0 in stage 7.0 (TID 311) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:07 WARN BlockManager: Putting block rdd_49_10 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:07 WARN BlockManager: Block rdd_49_10 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:07 WARN TaskSetManager: Lost task 10.0 in stage 7.0 (TID 315) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:07 WARN BlockManager: Putting block rdd_49_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:07 WARN BlockManager: Block rdd_49_4 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:07 WARN TaskSetManager: Lost task 4.0 in stage 7.0 (TID 309) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:07 WARN PythonRunner: Incomplete task 16.0 in stage 7 (TID 321) interrupted: Attempting to kill Python Worker\n",
      "21/11/30 15:59:07 WARN PythonRunner: Incomplete task 23.0 in stage 7 (TID 328) interrupted: Attempting to kill Python Worker\n",
      "21/11/30 15:59:07 WARN BlockManager: Putting block rdd_49_16 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:07 WARN BlockManager: Block rdd_49_16 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:07 WARN BlockManager: Putting block rdd_49_23 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:07 WARN BlockManager: Block rdd_49_23 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:07 WARN PythonRunner: Incomplete task 0.0 in stage 7 (TID 305) interrupted: Attempting to kill Python Worker\n",
      "21/11/30 15:59:07 WARN PythonRunner: Incomplete task 22.0 in stage 7 (TID 327) interrupted: Attempting to kill Python Worker\n",
      "21/11/30 15:59:07 WARN PythonRunner: Incomplete task 5.0 in stage 7 (TID 310) interrupted: Attempting to kill Python Worker\n",
      "21/11/30 15:59:07 WARN BlockManager: Putting block rdd_49_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:07 WARN BlockManager: Putting block rdd_49_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:07 WARN BlockManager: Putting block rdd_49_22 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:07 WARN BlockManager: Block rdd_49_22 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:07 WARN BlockManager: Block rdd_49_0 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:07 WARN BlockManager: Block rdd_49_5 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:07 WARN TaskSetManager: Lost task 22.0 in stage 7.0 (TID 327) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:08 WARN TaskSetManager: Lost task 5.0 in stage 7.0 (TID 310) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:08 WARN PythonRunner: Incomplete task 9.0 in stage 7 (TID 314) interrupted: Attempting to kill Python Worker\n",
      "21/11/30 15:59:08 WARN PythonRunner: Incomplete task 14.0 in stage 7 (TID 319) interrupted: Attempting to kill Python Worker\n",
      "21/11/30 15:59:08 WARN PythonRunner: Incomplete task 11.0 in stage 7 (TID 316) interrupted: Attempting to kill Python Worker\n",
      "21/11/30 15:59:08 WARN BlockManager: Putting block rdd_49_11 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:08 WARN BlockManager: Putting block rdd_49_14 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:08 WARN BlockManager: Block rdd_49_14 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:08 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 305) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:08 WARN BlockManager: Putting block rdd_49_9 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:08 WARN PythonRunner: Incomplete task 18.0 in stage 7 (TID 323) interrupted: Attempting to kill Python Worker\n",
      "21/11/30 15:59:08 WARN BlockManager: Block rdd_49_9 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:08 WARN PythonRunner: Incomplete task 20.0 in stage 7 (TID 325) interrupted: Attempting to kill Python Worker\n",
      "21/11/30 15:59:08 WARN BlockManager: Putting block rdd_49_18 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:08 WARN BlockManager: Block rdd_49_18 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:08 WARN BlockManager: Block rdd_49_11 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:08 WARN TaskSetManager: Lost task 14.0 in stage 7.0 (TID 319) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:08 WARN TaskSetManager: Lost task 23.0 in stage 7.0 (TID 328) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:08 WARN BlockManager: Putting block rdd_49_20 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:08 WARN BlockManager: Block rdd_49_20 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:08 WARN TaskSetManager: Lost task 16.0 in stage 7.0 (TID 321) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:08 WARN TaskSetManager: Lost task 11.0 in stage 7.0 (TID 316) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:08 WARN TaskSetManager: Lost task 20.0 in stage 7.0 (TID 325) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:08 WARN TaskSetManager: Lost task 18.0 in stage 7.0 (TID 323) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:08 WARN TaskSetManager: Lost task 9.0 in stage 7.0 (TID 314) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:08 WARN PythonRunner: Incomplete task 19.0 in stage 7 (TID 324) interrupted: Attempting to kill Python Worker\n",
      "21/11/30 15:59:08 WARN PythonRunner: Incomplete task 13.0 in stage 7 (TID 318) interrupted: Attempting to kill Python Worker\n",
      "21/11/30 15:59:08 WARN BlockManager: Putting block rdd_49_19 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:08 WARN BlockManager: Block rdd_49_19 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:08 WARN BlockManager: Putting block rdd_49_13 failed due to exception org.apache.spark.TaskKilledException.\n",
      "21/11/30 15:59:08 WARN BlockManager: Block rdd_49_13 could not be removed as it was not found on disk or in memory\n",
      "21/11/30 15:59:08 WARN TaskSetManager: Lost task 19.0 in stage 7.0 (TID 324) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n",
      "21/11/30 15:59:08 WARN TaskSetManager: Lost task 13.0 in stage 7.0 (TID 318) (69abb479a5e1 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "#Sobreescribir el dataset de filtro de sentencias con las nuevas columnas\n",
    "estructurales.seleccion.guardarDatos(parquet_file='extracciones3', partition_cols=['anno'], borrar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8213bb82-e5ad-44f9-a563-86c1706d8989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/30 15:37:53 WARN MemoryStore: Not enough space to cache rdd_20_8 in memory! (computed 69.0 MiB so far)\n",
      "21/11/30 15:37:53 WARN MemoryStore: Not enough space to cache rdd_20_14 in memory! (computed 68.6 MiB so far)\n",
      "21/11/30 15:37:53 WARN MemoryStore: Not enough space to cache rdd_20_5 in memory! (computed 68.8 MiB so far)\n",
      "21/11/30 15:37:53 WARN MemoryStore: Not enough space to cache rdd_20_3 in memory! (computed 69.4 MiB so far)\n",
      "21/11/30 15:37:53 WARN MemoryStore: Not enough space to cache rdd_20_26 in memory! (computed 68.4 MiB so far)\n",
      "21/11/30 15:37:53 WARN MemoryStore: Not enough space to cache rdd_20_28 in memory! (computed 68.5 MiB so far)\n",
      "21/11/30 15:37:53 WARN MemoryStore: Not enough space to cache rdd_20_27 in memory! (computed 68.7 MiB so far)\n",
      "21/11/30 15:37:53 WARN MemoryStore: Not enough space to cache rdd_20_25 in memory! (computed 69.1 MiB so far)\n",
      "21/11/30 15:37:53 WARN MemoryStore: Not enough space to cache rdd_20_29 in memory! (computed 68.3 MiB so far)\n",
      "21/11/30 15:37:53 WARN MemoryStore: Not enough space to cache rdd_20_30 in memory! (computed 68.8 MiB so far)\n",
      "21/11/30 15:37:53 WARN MemoryStore: Not enough space to cache rdd_20_6 in memory! (computed 133.4 MiB so far)\n",
      "21/11/30 15:37:53 WARN MemoryStore: Not enough space to cache rdd_20_32 in memory! (computed 67.9 MiB so far)\n",
      "21/11/30 15:37:54 WARN MemoryStore: Not enough space to cache rdd_20_7 in memory! (computed 263.3 MiB so far)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63446"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = estructurales.seleccion.sdf\n",
    "\n",
    "s.filter(s.plazosDefinidos.isNotNull()).count()\n",
    "# s.select('plazosDefinidos').dropna().count()\n",
    "# Cantidad anterior 30856\n",
    "# Cantidad con el reprocesamiento 63446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e5746d-00f6-4d44-84ca-e4237537a989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['archivo', 'txt', 'cleanText', 'num', 'fechahora_ext', 'redactor_ext',\n",
       "       'tipoResolucion_ext', 'termino_ext', 'voto_salvado', 'tema',\n",
       "       'Recurrente_ents', 'Cantón_ents', 'Fecha_ents', 'Constitución_ents',\n",
       "       'Redactor_ents', 'Ley_ents', 'EntidadPublica_ents', 'tema_prob',\n",
       "       'termino_lst', 'expediente', 'Fechahora_ents', 'CitaSentencias_ents',\n",
       "       'Recurrido_ents', 'Reglamento_ents', 'Magistrado_ents', '_id', 'anno',\n",
       "       'seguimiento', 'se_ordena', 'se_condena', 'plan', 'plazo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estructurales.seleccion.sdf.limit(10).toPandas().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70ca9a82-cad9-4054-a5bb-44de72710053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['archivo',\n",
       " 'txt',\n",
       " 'cleanText',\n",
       " 'num',\n",
       " 'fechahora_ext',\n",
       " 'redactor_ext',\n",
       " 'tipoResolucion_ext',\n",
       " 'termino_ext',\n",
       " 'voto_salvado',\n",
       " 'tema',\n",
       " 'Recurrente_ents',\n",
       " 'Cantón_ents',\n",
       " 'Fecha_ents',\n",
       " 'Constitución_ents',\n",
       " 'Redactor_ents',\n",
       " 'Ley_ents',\n",
       " 'EntidadPublica_ents',\n",
       " 'tema_prob',\n",
       " 'termino_lst',\n",
       " 'expediente',\n",
       " 'Fechahora_ents',\n",
       " 'CitaSentencias_ents',\n",
       " 'Recurrido_ents',\n",
       " 'Reglamento_ents',\n",
       " 'Magistrado_ents',\n",
       " '_id',\n",
       " 'seguimiento',\n",
       " 'se_ordena',\n",
       " 'se_condena',\n",
       " 'plan',\n",
       " 'plazo',\n",
       " 'se_ordena_PER',\n",
       " 'se_ordena_LOC',\n",
       " 'se_ordena_ORG',\n",
       " 'se_ordena_MISC',\n",
       " 'se_ordena_GPE',\n",
       " 'se_ordena_Ent_Pub',\n",
       " 'se_condena_PER',\n",
       " 'se_condena_LOC',\n",
       " 'se_condena_ORG',\n",
       " 'se_condena_MISC',\n",
       " 'se_condena_GPE',\n",
       " 'se_condena_Ent_Pub',\n",
       " 'extension_sentencia',\n",
       " 'extension_por_lo_tanto',\n",
       " 'plazosDefinidos',\n",
       " 'FechaSolicitud',\n",
       " 'num_resolucion',\n",
       " 'inst_internacionales',\n",
       " 'derechos_Norm',\n",
       " 'derechos_GenXPat',\n",
       " 'derechos_Acotados',\n",
       " 'derechos_General',\n",
       " 'derechos_Fundamental',\n",
       " 'derechos_Humano',\n",
       " 'citasFechas',\n",
       " 'id_sentencia',\n",
       " 'num_doc_oficial',\n",
       " 'expediente_oficial',\n",
       " 'anno']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('./datasets/estructurales2/extracciones2').filter('termino_ext == \"Con lugar\" OR  termino_ext == \"Con lugar parcial\" ').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be285599-df3e-4e3e-9a91-8e9e594e6386",
   "metadata": {},
   "source": [
    "# Cargar procesamiento de sentencias estructurales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b59d71cb-5bc5-46d5-ac7f-76320998da41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['archivo',\n",
       " 'txt',\n",
       " 'cleanText',\n",
       " 'num',\n",
       " 'fechahora_ext',\n",
       " 'redactor_ext',\n",
       " 'tipoResolucion_ext',\n",
       " 'termino_ext',\n",
       " 'voto_salvado',\n",
       " 'tema',\n",
       " 'Recurrente_ents',\n",
       " 'Cantón_ents',\n",
       " 'Fecha_ents',\n",
       " 'Constitución_ents',\n",
       " 'Redactor_ents',\n",
       " 'Ley_ents',\n",
       " 'EntidadPublica_ents',\n",
       " 'tema_prob',\n",
       " 'termino_lst',\n",
       " 'expediente',\n",
       " 'Fechahora_ents',\n",
       " 'CitaSentencias_ents',\n",
       " 'Recurrido_ents',\n",
       " 'Reglamento_ents',\n",
       " 'Magistrado_ents',\n",
       " '_id',\n",
       " 'anno']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminos = {\n",
    "    'seguimiento': [r'\\bseguimiento\\b'],\n",
    "    'se ordena': [r'\\bse ordena\\b'],\n",
    "    'plan': [r'\\bplan\\b'],\n",
    "    'plazo': [r'\\bplazo\\b']\n",
    "}\n",
    "seleccion = Seleccion(terminos, spark, parquet_path='../../src/datasets/complete', datasets_path='./datasets/estructurales2/extracciones2')\n",
    "seleccion.cargarPreprocesados()\n",
    "\n",
    "estructurales = SentenciasEstructurales(seleccion)\n",
    "seleccion.sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9425b42-ff43-44ab-951b-b9c77e6f2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "columnas = {\n",
    "    'num resolucion' : StringType()\n",
    "}\n",
    "\n",
    "estructurales.extraerNumeroSentencia(columnas, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da6726-dbb8-407a-a037-531bad558316",
   "metadata": {},
   "source": [
    "# Zona de trabajo (ignorar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a5164-38c6-4f26-9f4b-91fd49fe2816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = estructurales.seleccion.sdf\n",
    "#print(solo_portanto(s.where(s.plazosDefinidos.isNotNull()).limit(15).toPandas().loc[1,'txt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2324dd-cf90-417b-85c3-22582484236d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(s.where(s.plazosDefinidos.isNotNull()).limit(15).toPandas().loc[1,'se_ordena_PER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78981adc-3160-46a4-9c1b-d8a7b3e687c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(s.where(s.plazosDefinidos.isNotNull()).limit(15).toPandas().loc[1,'plazosDefinidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5752b7-7af0-4dd9-b9ea-e3923e87fe8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(s.where(s.plazosDefinidos.isNotNull()).limit(15).toPandas().loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76042dbb-d914-4836-8bbe-06c9aef7bddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
