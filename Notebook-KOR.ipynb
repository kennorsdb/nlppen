{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f56ce3-6a94-4fc2-a051-945ebfb32480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a834852-b9fd-483c-93e6-3b80f3e879c5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2ec801-fdee-413e-aca9-8ee8e77e0cac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from nlppen.extraccion.utils.Txt2Numbers import Txt2Numbers\n",
    "from nlppen.analisis import Analisis\n",
    "from nlppen.seleccion import Seleccion\n",
    "from nlppen.spark_udfs import solo_portanto, spark_get_spacy\n",
    "from nlppen.sentencias_estructurales import SentenciasEstructurales\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106f23a-2bb1-483d-bc31-aa48b9a7db2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialize spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b819481b-6ee8-42c7-9add-c19ceba43563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/18 20:52:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/18 20:52:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://69abb479a5e1:4041'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Transforming sentences\")\n",
    "         .config(\"spark.num.executors\", \"1\")\n",
    "         .config(\"spark.executor.memory\", \"12g\")\n",
    "         .config(\"spark.executor.cores\", \"2\")\n",
    "         .config(\"spark.driver.memory\", \"10g\")\n",
    "         .config(\"spark.memory.offHeap.enabled\", True)\n",
    "         .config(\"spark.memory.offHeap.size\", \"64g\")\n",
    "         .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "         .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "         .config(\"spark.sql.legacy.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "         .getOrCreate())\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.addFile('/home/jovyan/Work/ej/paquetes/nlppen/nlppen.zip')\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab75fdc-6e47-4671-b06b-4109f7a2bc66",
   "metadata": {},
   "source": [
    "# Buscar terminos en la sección de por lo tanto de la sentencia y aplicar filtro del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a92cddd-4b24-4bdf-85d7-ceabe68d009d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/18 20:52:58 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "151269"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "anno_inf = 2010\n",
    "anno_sup = 2021\n",
    "\n",
    "terminos = {\n",
    "    'seguimiento': [r'\\bseguimiento\\b'],\n",
    "    'se ordena': [r'\\bse ordena\\b'],\n",
    "    'plan': [r'\\bplan\\b'],\n",
    "    'plazo': [r'\\bplazo\\b']\n",
    "}\n",
    "\n",
    "\n",
    "seleccion = Seleccion(terminos, spark, parquet_path='../../src/datasets/complete', datasets_path='./datasets/estructurales2', filtro=f'anno >= {anno_inf} AND anno < {anno_sup}')\n",
    "seleccion.filtrar_sentencias(preprocess=solo_portanto, keepRowEmpty=True, partition_cols=['anno'], precargar=False)\n",
    "\n",
    "estructurales = SentenciasEstructurales(seleccion)\n",
    "\n",
    "seleccion.sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b108727-4f70-4ec4-b389-4eb10908e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet('../../src/datasets/complete').filter('termino_ext == \"Con lugar\" OR  termino_ext == \"Con lugar parcial\" ').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b9686-78ed-4c53-9cf9-694c588b45e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Formar dataset de sentencias estructurales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4588f-3661-4f06-bcdd-d5b1720c3e92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "columnas = {\n",
    "    'se ordena PER' : ArrayType(StringType()),\n",
    "    'se ordena LOC' : ArrayType(StringType()),\n",
    "    'se ordena ORG' : ArrayType(StringType()),\n",
    "    'se ordena MISC' : ArrayType(StringType()),\n",
    "    'se ordena GPE' : ArrayType(StringType()),\n",
    "    'se ordena Ent Pub' : ArrayType(StringType())\n",
    "}\n",
    "estructurales.separarSeOrdena(columnas, True, True)\n",
    "\n",
    "columnas = {\n",
    "    'extension sentencia' : IntegerType(),\n",
    "    'extension por lo tanto' : IntegerType()\n",
    "}\n",
    "estructurales.extraerExtension(columnas, True)\n",
    "\n",
    "columnas = {\n",
    "    'plazosDefinidos' : ArrayType(TimestampType())\n",
    "}\n",
    "estructurales.plazosDefinidos(columnas, True)\n",
    "\n",
    "columnas = {\n",
    "    'FechaSolicitud' : TimestampType(),\n",
    "}\n",
    "\n",
    "estructurales.extrarFechaRecibido(columnas, True)\n",
    "\n",
    "columnas = {\n",
    "    'num resolucion' : StringType()\n",
    "}\n",
    "\n",
    "estructurales.extraerNumeroSentencia(columnas, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f0362-10e8-48c5-a441-1e1edaeebb4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "columnas = {\n",
    "    'inst internacionales' : ArrayType(StringType())\n",
    "}\n",
    "\n",
    "estructurales.extraerInstrumentosInternacionales(columnas, True)\n",
    "\n",
    "columnas = {\n",
    "    'derechos Norm' : ArrayType(StringType()),\n",
    "    'derechos GenXPat' : ArrayType(StringType())\n",
    "}\n",
    "estructurales.extraerDerechos(columnas, True)\n",
    "\n",
    "columnas = {\n",
    "    'derechos Acotados' : ArrayType(StringType()),\n",
    "    'derechos General' : ArrayType(StringType()),\n",
    "    'derechos Fundamental' : ArrayType(StringType()),\n",
    "    'derechos Humano' : ArrayType(StringType())\n",
    "}\n",
    "\n",
    "estructurales.extraerDerechosSinNormalizar(columnas, True)\n",
    "\n",
    "columnas = {\n",
    "        'id_sentencia' : StringType(),\n",
    "        'num_doc_oficial' : StringType(),\n",
    "        'expediente_oficial' : StringType()\n",
    "    }\n",
    "estructurales.agregarIDSentencia(columnas, sentencias, True)\n",
    "\n",
    "columnas = {\n",
    "        'citasVotDate' : MapType(StringType(), TimestampType()),\n",
    "        'citasIDVoto' : MapType(StringType(), StringType())\n",
    "    }\n",
    "    estructurales.extrarCitaSentenciasFecha(columnas, sentencias, True)\n",
    "#Sobreescribir el dataset de filtro de sentencias con las nuevas columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e158ac6-bbd8-4773-b327-29d2fba6f25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estructurales.seleccion.guardarDatos(parquet_file='extracciones2', partition_cols=['anno'], borrar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e5746d-00f6-4d44-84ca-e4237537a989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['archivo', 'txt', 'cleanText', 'num', 'fechahora_ext', 'redactor_ext',\n",
       "       'tipoResolucion_ext', 'termino_ext', 'voto_salvado', 'tema',\n",
       "       'Recurrente_ents', 'Cantón_ents', 'Fecha_ents', 'Constitución_ents',\n",
       "       'Redactor_ents', 'Ley_ents', 'EntidadPublica_ents', 'tema_prob',\n",
       "       'termino_lst', 'expediente', 'Fechahora_ents', 'CitaSentencias_ents',\n",
       "       'Recurrido_ents', 'Reglamento_ents', 'Magistrado_ents', '_id', 'anno',\n",
       "       'seguimiento', 'se_ordena', 'plan', 'plazo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estructurales.seleccion.sdf.limit(10).toPandas().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c6e7a-358d-4e10-ab15-31162736cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "estructurales.seleccion.sdf.dropNull('FechaSolicitud').groupby('anno').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4691309-2cb2-4bce-9325-532aba01ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "estructurales.seleccion.sdf.select('FechaSolicitud', 'anno').filter('FechaSolicitud is not Null').toPandas().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3311da56-481f-47eb-be25-f5db561e0408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estructurales.seleccion.sdf.limit(15).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be285599-df3e-4e3e-9a91-8e9e594e6386",
   "metadata": {},
   "source": [
    "# Cargar procesamiento de sentencias estructurales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b59d71cb-5bc5-46d5-ac7f-76320998da41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o113.parquet.\n: java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Suspicious paths:\b\n\tfile:/home/jovyan/work/ej/paquetes/nlppen/datasets/estructurales/terminos.parquet_prev\n\tfile:/home/jovyan/work/ej/paquetes/nlppen/datasets/estructurales/extracciones2\n\tfile:/home/jovyan/work/ej/paquetes/nlppen/datasets/estructurales/terminos.parquet\n\tfile:/home/jovyan/work/ej/paquetes/nlppen/datasets/estructurales2/terminos.parquet\n\nIf provided paths are partition directories, please set \"basePath\" in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.\n\tat scala.Predef$.assert(Predef.scala:223)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:172)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:104)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:158)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:73)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:418)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:834)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2174/2274089397.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'plazo'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mr'\\bplazo\\b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mseleccion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeleccion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterminos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparquet_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./datasets/estructurales2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mseleccion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcargarPreprocesados\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/ej/paquetes/nlppen/nlppen/seleccion.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, terminos, spark, parquet_path, cambios_path, datasets_path, filtro)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiltro_carga\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             self.sdf = self.spark.read.parquet(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    456\u001b[0m                        modifiedAfter=modifiedAfter)\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o113.parquet.\n: java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Suspicious paths:\b\n\tfile:/home/jovyan/work/ej/paquetes/nlppen/datasets/estructurales/terminos.parquet_prev\n\tfile:/home/jovyan/work/ej/paquetes/nlppen/datasets/estructurales/extracciones2\n\tfile:/home/jovyan/work/ej/paquetes/nlppen/datasets/estructurales/terminos.parquet\n\tfile:/home/jovyan/work/ej/paquetes/nlppen/datasets/estructurales2/terminos.parquet\n\nIf provided paths are partition directories, please set \"basePath\" in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.\n\tat scala.Predef$.assert(Predef.scala:223)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:172)\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:104)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:158)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:73)\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:50)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:418)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:834)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "terminos = {\n",
    "    'seguimiento': [r'\\bseguimiento\\b'],\n",
    "    'se ordena': [r'\\bse ordena\\b'],\n",
    "    'plan': [r'\\bplan\\b'],\n",
    "    'plazo': [r'\\bplazo\\b']\n",
    "}\n",
    "seleccion = Seleccion(terminos, spark, parquet_path='./datasets', datasets_path='./datasets/estructurales2')\n",
    "seleccion.cargarPreprocesados()\n",
    "\n",
    "estructurales = SentenciasEstructurales(seleccion)\n",
    "seleccion.sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9425b42-ff43-44ab-951b-b9c77e6f2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "columnas = {\n",
    "    'num resolucion' : StringType()\n",
    "}\n",
    "\n",
    "estructurales.extraerNumeroSentencia(columnas, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da6726-dbb8-407a-a037-531bad558316",
   "metadata": {},
   "source": [
    "# Zona de trabajo (ignorar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a5164-38c6-4f26-9f4b-91fd49fe2816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = estructurales.seleccion.sdf\n",
    "#print(solo_portanto(s.where(s.plazosDefinidos.isNotNull()).limit(15).toPandas().loc[1,'txt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2324dd-cf90-417b-85c3-22582484236d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(s.where(s.plazosDefinidos.isNotNull()).limit(15).toPandas().loc[1,'se_ordena_PER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78981adc-3160-46a4-9c1b-d8a7b3e687c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(s.where(s.plazosDefinidos.isNotNull()).limit(15).toPandas().loc[1,'plazosDefinidos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5752b7-7af0-4dd9-b9ea-e3923e87fe8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(s.where(s.plazosDefinidos.isNotNull()).limit(15).toPandas().loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76042dbb-d914-4836-8bbe-06c9aef7bddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
